# -*- coding: utf-8 -*-
"""fact_checker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CTDF3_NxXRoRh_27efsQst73eDe9UrJG
"""

from sentence_transformers import SentenceTransformer, util
import numpy as np

# 🔍 Load the SBERT model
model = SentenceTransformer('all-MiniLM-L6-v2')  # small, fast, effective

# ✅ Semantic similarity-based fact checker
def fact_check_claim_with_articles(claim: str, related_articles: list) -> dict:
    try:
        # Encode claim and related articles
        claim_embedding = model.encode(claim, convert_to_tensor=True)
        article_embeddings = model.encode(related_articles, convert_to_tensor=True)

        # Compute cosine similarities
        similarities = util.cos_sim(claim_embedding, article_embeddings)[0].cpu().numpy()
        average_similarity = np.mean(similarities)

        # ✅ Loosened Verdict thresholds for better true detection
        if average_similarity >= 0.68:
            verdict = "supported"
            score = 0.9
        elif average_similarity <= 0.3:
            verdict = "contradicted"
            score = 0.2
        else:
            verdict = "unverifiable"
            score = 0.5

        return {
            "claim": claim,
            "verdict": verdict,
            "score": round(score, 2),
            "reason": f"Average semantic similarity: {round(average_similarity, 3)} across {len(related_articles)} articles."
        }

    except Exception as e:
        return {
            "claim": claim,
            "verdict": "error",
            "score": 0.0,
            "reason": str(e)
        }

# 🔁 For multiple claims
def fact_check_multiple_claims(claims: list, all_articles: list[list]) -> list:
    results = []
    for i, claim in enumerate(claims):
        related = all_articles[i] if i < len(all_articles) else []
        results.append(fact_check_claim_with_articles(claim, related))
    return results

